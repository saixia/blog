<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Tensor,Vector Matrix,深度学习,deep learn," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.0" />






<meta name="description" content="简介目的: 本节主要讲述反向传播和一种采用链式法则计算梯度的方法。理解反向传播的处理过程对于理解、开发、设计、调试神经网络至关重要。 问题描述: 本节中主要解决已知函数 $f(x)$，求函数在 $x$ 的梯度，其中 $x$ 为输入向量。 目的： 回想一下神经网络这个特定问题中,其中 $f$ 相当于损失函数 $L(x)$ ,输入参数 $x$ 对应训练数据和神经网络中的权重。例如，损失函数是SVM损失">
<meta name="keywords" content="Tensor,Vector Matrix,深度学习,deep learn">
<meta property="og:type" content="article">
<meta property="og:title" content="Vector Matrix and Tensor Derivatives">
<meta property="og:url" content="http://saixia.me/2017/08/06/Vector Matrix and Tensor Derivatives/index.html">
<meta property="og:site_name" content="Seth&#39;s blog">
<meta property="og:description" content="简介目的: 本节主要讲述反向传播和一种采用链式法则计算梯度的方法。理解反向传播的处理过程对于理解、开发、设计、调试神经网络至关重要。 问题描述: 本节中主要解决已知函数 $f(x)$，求函数在 $x$ 的梯度，其中 $x$ 为输入向量。 目的： 回想一下神经网络这个特定问题中,其中 $f$ 相当于损失函数 $L(x)$ ,输入参数 $x$ 对应训练数据和神经网络中的权重。例如，损失函数是SVM损失">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://saixia.me/2017/08/06/Vector%20Matrix%20and%20Tensor%20Derivatives/images/Vector%20Matrix%20and%20Tensor%20Derivatives/graph1.jpg">
<meta property="og:image" content="http://saixia.me/2017/08/06/Vector%20Matrix%20and%20Tensor%20Derivatives/images/Vector%20Matrix%20and%20Tensor%20Derivatives/graph2.jpg">
<meta property="og:image" content="http://saixia.me/2017/08/06/Vector%20Matrix%20and%20Tensor%20Derivatives/images/Vector%20Matrix%20and%20Tensor%20Derivatives/graph3.jpg">
<meta property="og:updated_time" content="2017-08-26T06:34:03.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Vector Matrix and Tensor Derivatives">
<meta name="twitter:description" content="简介目的: 本节主要讲述反向传播和一种采用链式法则计算梯度的方法。理解反向传播的处理过程对于理解、开发、设计、调试神经网络至关重要。 问题描述: 本节中主要解决已知函数 $f(x)$，求函数在 $x$ 的梯度，其中 $x$ 为输入向量。 目的： 回想一下神经网络这个特定问题中,其中 $f$ 相当于损失函数 $L(x)$ ,输入参数 $x$ 对应训练数据和神经网络中的权重。例如，损失函数是SVM损失">
<meta name="twitter:image" content="http://saixia.me/2017/08/06/Vector%20Matrix%20and%20Tensor%20Derivatives/images/Vector%20Matrix%20and%20Tensor%20Derivatives/graph1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":"ture"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://saixia.me/2017/08/06/Vector Matrix and Tensor Derivatives/"/>





  <title> Vector Matrix and Tensor Derivatives | Seth's blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  











  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1262332391&web_id=1262332391" language="JavaScript"></script>
  </div>






  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Seth's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Seth</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://saixia.me/2017/08/06/Vector Matrix and Tensor Derivatives/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="saixia">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Seth's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Vector Matrix and Tensor Derivatives
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-06T16:19:29+08:00">
                2017-08-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deep-learning/" itemprop="url" rel="index">
                    <span itemprop="name">deep learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-comment-o"></i>
              </span>
              
                <a href="/2017/08/06/Vector Matrix and Tensor Derivatives/#SOHUCS" itemprop="discussionUrl">
                  <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2017/08/06/Vector Matrix and Tensor Derivatives/" itemprop="commentsCount"></span>
                </a>
              
            
          

          
          
             <span id="/2017/08/06/Vector Matrix and Tensor Derivatives/" class="leancloud_visitors" data-flag-title="Vector Matrix and Tensor Derivatives">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><strong>目的:</strong> 本节主要讲述反向传播和一种采用链式法则计算梯度的方法。理解反向传播的处理过程对于理解、开发、设计、调试神经网络至关重要。</p>
<p><strong>问题描述:</strong> 本节中主要解决已知函数 $f(x)$，求函数在 $x$ 的梯度，其中 $x$ 为输入向量。</p>
<p><strong>目的：</strong> 回想一下神经网络这个特定问题中,其中 $f$ 相当于损失函数 $L(x)$ ,输入参数 $x$ 对应训练数据和神经网络中的权重。例如，损失函数是SVM损失函数，输入为训练数据 $\left (x_{i},y_{j} \right )$ , $i=1 \cdots N$ 和网络的权重 $W$ 和偏置 $b$ 。请注意通常在机器学习中我们认为训练数据是给定，权重是一个可控的变量。因此，我们采用反向传播可以非常容易求得输入$x$的梯度。在训练过程中，只需要计算参数 $W$, $b$ 的梯度，所以我们可以采用它来执行参数的迭代。然而，在后续课程中$x_i$的梯度在某些情况下同样有用，例如：用于可视化和解释神经网络的执行过程。</p>
<p>如果您在参加本课程前已经了解了链式法则，我们乃鼓励您阅读本文。因为，本文以图解的形式讲解了反向传播的整个过程，对于理解反向传播能有一个直观的理解。</p>
<h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>熟悉简单实例后，我们就能处理复杂函数求梯度。以两个变量相乘为例$f(x,y)=xy$，$x$， $y$ 的偏导分表为：</p>
<p>$$f(x,y)=xy\rightarrow\frac{\partial f}{\partial x}=y\qquad\frac{\partial f}{\partial y}=x$$</p>
<p>注解：请记住以下的演算。以下变化率标示函数在值附近无限小的区域内的变化率（如：一元函数的斜率）。</p>
<p>$$\frac{df(x)}{dx} =\lim_{h \rightarrow 0}\frac{f(x+h)-f(x)}{h} $$</p>
<p>采用等式左侧的符号表示函数 $f(x)$ 在 $x$ 处的微分。当 $h$ 比非常小时，上述表达是标示函数在此处的斜率。也就是说每个变量的导数标示整个表达式对于该变量的敏感度。例如：if $x=4$，$y=-3$ then $f(x,y)=-12$,此时函数 $f(x)$ 在 $x$ 处的微分为-3.这个表示，如果变量 $x$ 增加一小点，函数 $f(x)$ 的值将减少3倍。这个可以通过以下表达是看出$f(x,y)=f(x)+h*\frac{df(x)}{dx}$,类似的$\frac{df(x)}{dy}=4$,我们对 $h$ 增加一小点，函数的输出将增加 $4h$。</p>
<blockquote>
<p>每个变量的导数标示目标函数在此处的变化快慢。</p>
</blockquote>
<p>根据上述，偏导$\Delta f$是一个$\Delta f=[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]=[y,x]$的向量。尽管梯度是一个向量，本文中为简述表达我们采用梯度 $x$ 标示偏导向量 $x$。</p>
<p>我们可以求得两个变量相加的偏导：</p>
<p>$$f(x)=x+y\rightarrow\frac{\partial f}{\partial x}=1\qquad\frac{\partial f}{\partial y}=1$$</p>
<p>这个实例中，不管 $x$ 和 $y$ 是取什么值，函数对 $x$ 和 $y$的偏导都是1。也就是说 $x$ 、$y$ 的增长都会导致函数 $f$值的增长，并且这个值的增长比率只于实际的值 $x$ 和 $y$ 有关系（这个与两个变量的乘不一样）。最后一个case我们讨论Max操作：</p>
<p>$$f(x)=max(x,y)\rightarrow\frac{\partial f}{\partial x}=1(x&gt;=y)\qquad\frac{\partial f}{\partial y}=1(y&gt;=x)$$</p>
<p>这个函数中，两个变量较大的值的偏导为1，另外一个则为0。例如：当 $x=4$， $y=2$时，最大值为 $4$，这个函数在此处对于$y$值不敏感，也就是说，当我们在此处增加一个很小的$h$值函数还是输出4，对结果无影响，因此它的梯度为0。当然，当我们将$y$的值增加非常大时，将会对结果函数值产生影响，上述这种情况仅当$\lim_{h\to 0}$时候成立.</p>
<h1 id="复合连锁法则"><a href="#复合连锁法则" class="headerlink" title="复合连锁法则"></a>复合连锁法则</h1><p>现在开始讲解更复杂的表达式的求解，目标函数由多个函数的组合，例如：$f(x,y,z)=(x+y)x$。这个表达式依然相对简单，我们采用它帮助直观我们理解反向传播。在这个例子中，我们将函数分解为两个表达式：$q=x+y$,$f=q*z$。在前面已经讲解过着两个函数如何求偏导。函数 $f$ 是 $q$ 和 $z$ 变量的乘积，所以，他们的偏导分别为$\frac{\partial f}{\partial q}=z$, $\frac{\partial f}{\partial z}=q$，其中 $q$为 $x$ 和 $y$ 的和，他们的$\frac{\partial q}{\partial x}=1$，$ \frac{\partial q}{\partial y}=1$。然而，在我们的使用中，我们无需关系 $q$ 值的偏导$\frac{\partial f}{\partial q}$的值，这个值在本应用场景没有用。我们只关系函数 $f$在值 $x$， $y$ ， $z$ 处的偏导。链式法则可以求得多个“链式”表达式的梯度。例如：$\frac{\partial f}{\partial x}=\frac{\partial f}{\partial q}\frac{\partial q}{\partial x}$。本例中我们可以通过两个偏导的乘积求得函数 $f$在 $x$的偏导。让我们来看一个python代码写的例子：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># set some inputs</span></div><div class="line">x = <span class="number">-2</span>; y = <span class="number">5</span>; z = <span class="number">-4</span></div><div class="line"></div><div class="line"><span class="comment"># perform the forward pass</span></div><div class="line">q = x + y <span class="comment"># q becomes 3</span></div><div class="line">f = q * z <span class="comment"># f becomes -12</span></div><div class="line"></div><div class="line"><span class="comment"># perform the backward pass (backpropagation) in reverse order:</span></div><div class="line"><span class="comment"># first backprop through f = q * z</span></div><div class="line">dfdz = q <span class="comment"># df/dz = q, so gradient on z becomes 3</span></div><div class="line">dfdq = z <span class="comment"># df/dq = z, so gradient on q becomes -4</span></div><div class="line"><span class="comment"># now backprop through q = x + y</span></div><div class="line">dfdx = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dx = 1. And the multiplication here is the chain rule!</span></div><div class="line">dfdy = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dy = 1</span></div></pre></td></tr></table></figure>
<p>最后我们将梯度赋值给变量 $[dfdx, dfdy, dfdz]$, 这些值告诉我们函数 $f$ 对于 $x$， $y$， $z$的敏感度。这个简单的例子阐述了反向传播的过程。今后我们将采用更加简洁的写法，因此我们不使用 $df$。这个例子中使用 $dfdq$ 表示 $dq$梯度的最终输出。</p>
<p>上面的计算过程通过下图图解：</p>
<img src="/2017/08/06/Vector Matrix and Tensor Derivatives/images/Vector Matrix and Tensor Derivatives/graph1.jpg" class="full-image">
<h1 id="直观理解反向传播"><a href="#直观理解反向传播" class="headerlink" title="直观理解反向传播"></a>直观理解反向传播</h1><p>反向传播是一个优美的本地处理过程。图中每个计算单元获得输入后，可以立即算出输出值和当前梯度（local gradient）。值得注意的是，计算过程无需关系全图中的其他值，就可以计算出当前梯度。当全部的前向传播计算完以后，逆向传播就可以根据前向传播的值计算梯度。采用链式法则，可以计算出所有输入的偏导。</p>
<blockquote>
<p>神经网络中每个计算单元也可以采用链式法则求得输出值和每个变量的偏导。  </p>
</blockquote>
<p>接下来，让我们解释一下以上的例子。加法处理单元的输入 $[-2, 5]$ 计算结果为 $3$。加法处理计算完以后，接着输入每个变量的当前梯度$+1$ 。上图最终的输出结果为 $-12$。在反向传播的过程中运用链式法则求得加法计算单元的梯度为 $-4$。为表述直观，我们将每个单元的计算结果放在横线上，将反向传播的求得的结果放在横线下方。根据上面的计算方法，求得当前单元的梯度 $-4$。继续回退，就可以求得 $x$ 和 $y$ 的梯度（1*-4=-4）。有上计算可以获得，如果x、y的值降低，他们的和也降低，这个使得他们的乘积增加。</p>
<p>反向传播可以理解为每个计算单元之间的作用使得最终结果是增大或减小（他们的增幅），如此影响最终的输出结果。</p>
<h1 id="Sigmod函数"><a href="#Sigmod函数" class="headerlink" title="Sigmod函数"></a>Sigmod函数</h1><p>上述计算单元可以是任意的处理函数。每个不同的函数都可以当成一个处理单元，然后再将多个处理单元组合为一个大的处理单元（神经网络中就是有一个个小的处理单元组合而成），为了方便运算，我们可以对一个复杂处理进行分解。接下来我们以Sigmod函数为例分析如何拆解：</p>
<p>$$f(W,x)=\frac{1}{1+e^{w0x0+w1x1+w2}}$$</p>
<p>在随后的课程中，Sigmod激活函数是一个2维的核函数。在此先把它当做一个输入为 $w$，$x$的函数。这个函数有多个处理单元组成，这些加法、乘法、max处理单元以及在上面进行了讲解。</p>
<p>$$f(x)=\frac{1}{x}\rightarrow\frac{\partial f}{\partial x}=-1/x^2$$</p>
<p>$$f(x)=e^x\rightarrow\frac{\partial f}{\partial x}=e^x$$</p>
<p>$$f_a(x)=ax\rightarrow\frac{\partial f}{\partial x}=a$$</p>
<p>函数$f_a$分别表示上式中的常数 $c$ 和 $a$。这可以当做已有一个输入的特殊的加法和乘法处理单元。在整个处理过程如下：</p>
<img src="/2017/08/06/Vector Matrix and Tensor Derivatives/images/Vector Matrix and Tensor Derivatives/graph2.jpg" class="full-image">
<p>由上处理可知，一个长的链式处理函数可以通过 $w$，$x$之间的点乘获得计算结果。这个函数被称作为Sigmoid函数$\partial (x)$.这个函数的偏导可以采用一下方法求得：</p>
<p>$$\partial (x)=\frac{1}{1+e^x}$$</p>
<p>$$\rightarrow\frac{d\partial(x)}{dx}=\frac{e^{-x}}{(1+e^{-x})^2}=(\frac{1+e^{-x}-1}{1+e^{-x}})(\frac{1}{1+e^{-x}})=(1-\partial(x))(\partial(x))$$</p>
<p>采用以上等式求函数的偏导变得非常简单。例如：sigmod函数输入 $1.0$在正向传播的时候输出 $0.73$。跟进这个表达是可以很快求得偏导 $（1-0.73）*0.73~=0.2$。接下来看一下核函数求偏导的处理代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">w = [<span class="number">2</span>,<span class="number">-3</span>,<span class="number">-3</span>] <span class="comment"># assume some random weights and data</span></div><div class="line">x = [<span class="number">-1</span>, <span class="number">-2</span>]</div><div class="line"></div><div class="line"><span class="comment"># forward pass</span></div><div class="line">dot = w[<span class="number">0</span>]*x[<span class="number">0</span>] + w[<span class="number">1</span>]*x[<span class="number">1</span>] + w[<span class="number">2</span>]</div><div class="line">f = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-dot)) <span class="comment"># sigmoid function</span></div><div class="line"></div><div class="line"><span class="comment"># backward pass through the neuron (backpropagation)</span></div><div class="line">ddot = (<span class="number">1</span> - f) * f <span class="comment"># gradient on dot variable, using the sigmoid gradient derivation</span></div><div class="line">dx = [w[<span class="number">0</span>] * ddot, w[<span class="number">1</span>] * ddot] <span class="comment"># backprop into x</span></div><div class="line">dw = [x[<span class="number">0</span>] * ddot, x[<span class="number">1</span>] * ddot, <span class="number">1.0</span> * ddot] <span class="comment"># backprop into w</span></div><div class="line"><span class="comment"># we're done! we have the gradients on the inputs to the circuit</span></div></pre></td></tr></table></figure>
<p>注：反向传播阶段。采用dot标示输入 $w$和 $x$的点乘。反向传播的过程中我们计算出每个变量的梯度。</p>
<p>本节我们主要讲一种方便使用的方法计算正向传播与反向传播。在此基础上就可以直观的了解正向传播和反向传播的计算过程。本章还提供了计算代码。</p>
<h1 id="反向传播实例：计算过程"><a href="#反向传播实例：计算过程" class="headerlink" title="反向传播实例：计算过程"></a>反向传播实例：计算过程</h1><p>接下来我们学习更复杂的实例:</p>
<p>$$f(x,y)=\frac{x+\sigma(y)}{\sigma(x)+(x+y)^2}$$</p>
<p>很显然，函数非常适合与用作方向传播的练习。应为我们采用之前的方法对 $x$ 和 $y$进行分解求偏导将非常复杂。然而，实时事实证明完全没有必要这样做，我们无需一个明确的函数来表示这个演变过程，我们只需要知道如何计算它。以下是正向传播的演变过程：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">x = <span class="number">3</span> <span class="comment"># example values</span></div><div class="line">y = <span class="number">-4</span></div><div class="line"></div><div class="line"><span class="comment"># forward pass</span></div><div class="line">sigy = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-y)) <span class="comment"># sigmoid in numerator   #(1)</span></div><div class="line">num = x + sigy <span class="comment"># numerator                               #(2)</span></div><div class="line">sigx = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-x)) <span class="comment"># sigmoid in denominator #(3)</span></div><div class="line">xpy = x + y                                              <span class="comment">#(4)</span></div><div class="line">xpysqr = xpy**<span class="number">2</span>                                          <span class="comment">#(5)</span></div><div class="line">den = sigx + xpysqr <span class="comment"># denominator                        #(6)</span></div><div class="line">invden = <span class="number">1.0</span> / den                                       <span class="comment">#(7)</span></div><div class="line">f = num * invden <span class="comment"># done!                                 #(8)</span></div></pre></td></tr></table></figure>
<p>以上是正向传播计算的整个过程，我们引进了多个中间变量结构化代码，每个中间表达式都简单可以求偏导。因此，反向传播就比较容易算了。整个反向传播的计算代码如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># backprop f = num * invden</span></div><div class="line">dnum = invden <span class="comment"># gradient on numerator                             #(8)</span></div><div class="line">dinvden = num                                                     <span class="comment">#(8)</span></div><div class="line"><span class="comment"># backprop invden = 1.0 / den </span></div><div class="line">dden = (<span class="number">-1.0</span> / (den**<span class="number">2</span>)) * dinvden                                <span class="comment">#(7)</span></div><div class="line"><span class="comment"># backprop den = sigx + xpysqr</span></div><div class="line">dsigx = (<span class="number">1</span>) * dden                                                <span class="comment">#(6)</span></div><div class="line">dxpysqr = (<span class="number">1</span>) * dden                                              <span class="comment">#(6)</span></div><div class="line"><span class="comment"># backprop xpysqr = xpy**2</span></div><div class="line">dxpy = (<span class="number">2</span> * xpy) * dxpysqr                                        <span class="comment">#(5)</span></div><div class="line"><span class="comment"># backprop xpy = x + y</span></div><div class="line">dx = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></div><div class="line">dy = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></div><div class="line"><span class="comment"># backprop sigx = 1.0 / (1 + math.exp(-x))</span></div><div class="line">dx += ((<span class="number">1</span> - sigx) * sigx) * dsigx <span class="comment"># Notice += !! See notes below  #(3)</span></div><div class="line"><span class="comment"># backprop num = x + sigy</span></div><div class="line">dx += (<span class="number">1</span>) * dnum                                                  <span class="comment">#(2)</span></div><div class="line">dsigy = (<span class="number">1</span>) * dnum                                                <span class="comment">#(2)</span></div><div class="line"><span class="comment"># backprop sigy = 1.0 / (1 + math.exp(-y))</span></div><div class="line">dy += ((<span class="number">1</span> - sigy) * sigy) * dsigy                                 <span class="comment">#(1)</span></div><div class="line"><span class="comment"># done! phew</span></div></pre></td></tr></table></figure>
<p>注意点：</p>
<ol>
<li>保留正向传播的计算结果，应为在反向传播过程中会使用到。但是如果太复杂了可以重新计算；  </li>
<li>累加：正向传播过程多次使用 $x$、 $y$ 计算过程中需要注意+=、=等操作符，避免覆盖之前的旧值；</li>
</ol>
<h1 id="反向传播模式"><a href="#反向传播模式" class="headerlink" title="反向传播模式"></a>反向传播模式</h1><p>有趣的是反向传播通常可以以一种直观的方式阐述。例如神经网络中常用的三种计算（add、mul、max），在反向传播过程中都可以简单的阐述。让我们看一下一下的例子：</p>
<img src="/2017/08/06/Vector Matrix and Tensor Derivatives/images/Vector Matrix and Tensor Derivatives/graph3.jpg" class="full-image">
<p>通过上图可知：<br><strong>加法操作</strong> 的梯度均匀分配给所有输入变量，不用管正向传播过程中的值。因为加法操作的局部梯度都是 $+1.0$，x*1.0保持不变，因此输出的梯度完全等于输入时的梯度。本例加法操作的梯度将2.0的局部梯度传递给两个输入变量，均保持不变；<br><strong>max操作</strong>与加法操作不一样，max操作将梯度传递给输入值大的。本例中因为 $1.0$大于 $-1$，所以在z出的梯度为 $2$，在w出的值为 $0$；<br><strong>乘法操作</strong>解释起来相对复杂一点。局部梯度输入值是根据链式法则来确定梯度的输出。在本例中 $x$的梯度为 $-4.00*2.00=-8.0$；</p>
<p>不直观的影响以及他们的影响。请注意，如果乘法操作的其中一个输入非常小，而另外一个输入非常大，那么乘法处理就会出现如下现象，它将为小的输入分配一个较大的梯度，并为一个大的数据分配一个较小的梯度。在线下分类器权重和输入点的乘积$W^TX_i$中，意味着数据集的大小对梯度有较大的影响。例如，如果在预处理期间将数据$X_i*1000$意味着梯度也将扩大1000倍，所以必须通过降低学习率来补偿。</p>
<h1 id="梯度计算中的向量运算"><a href="#梯度计算中的向量运算" class="headerlink" title="梯度计算中的向量运算"></a>梯度计算中的向量运算</h1><p>之前提的都是单个变量的处理，不过所有这些操作都适合于矩阵和向量计算。然是，在处理过程中需要注意维度。</p>
<p><strong>矩阵-矩阵的梯度</strong>最复杂的处理是矩阵与矩阵的乘积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># forward pass</span></div><div class="line">W = np.random.randn(<span class="number">5</span>, <span class="number">10</span>)</div><div class="line">X = np.random.randn(<span class="number">10</span>, <span class="number">3</span>)</div><div class="line">D = W.dot(X)</div><div class="line"></div><div class="line"><span class="comment"># now suppose we had the gradient on D from above in the circuit</span></div><div class="line">dD = np.random.randn(*D.shape) <span class="comment"># same shape as D</span></div><div class="line">dW = dD.dot(X.T) <span class="comment">#.T gives the transpose of the matrix</span></div><div class="line">dX = W.T.dot(dD)</div></pre></td></tr></table></figure>
<p>tips：使用维度分析！我们无需记住他们的维度，因为可以通过推导获得 $dW$和 $dx$ 的维度。例如，我们知道计算后求得权重的梯度 $dW$ 的维度必须和 $W$计算后的的维度，计算后的维度取决于 $x$ 和 $dD$的乘积。总是存在一种方法可以计算出维度，因此维度分析是有效的。例如，$x$和 $dD$ 分别为 $[10*3]$, $[5*3]$的矩阵，根据维度分析，我们可以求得 $dW$ 和 $w$ 的维度为 $[5*10]$ 可以通过$dD.dot(X.T)$求得。</p>
<p><strong>使用小的明确的例子</strong>有时很难求得复杂表达式的梯度。我们建议先将复杂表达是分解，然后在纸上推导出他们的求导过程。</p>
<p>Erik Learned-Miller撰写了一个关于矩阵和向量求导的资料，<a href="http://cs231n.stanford.edu/vecDerivs.pdf" target="_blank" rel="external">点击直达</a>。 </p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li>我们采用直观的流程图阐述梯度的计算过程，同时每个变量是如何影响函数的输出值。</li>
<li>我们讨论了分阶段计算反向传播值。我们可以先将负责函数分解为可以求导的小模块，然后采用链式法则求得梯度。至关重要的是您无需将求导函数在纸上进行推导，您就可以求得每个变量偏导。因此，将表达式进行分解后可以分阶段计算操作值，然后通过反向传播求得梯度。</li>
</ul>
<p>在下节中，我们将开始学习定义神经网络，通过反向传播我们可以计算神经网络损失函数的梯度。换句话说，我们已经具备训练审定网络的能力，并掌握了本课程中比较难懂的概念！</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">原英文地址：</a><br><a href="http://arxiv.org/abs/1502.05767" target="_blank" rel="external">Automatic differentiation in machine learning: a survey</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/images/wechatpay.jpeg" alt="saixia WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/images/alipay.jpeg" alt="saixia Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Tensor/" rel="tag"># Tensor</a>
          
            <a href="/tags/Vector-Matrix/" rel="tag"># Vector Matrix</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/deep-learn/" rel="tag"># deep learn</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/01/Flink Metrics/" rel="next" title="Flink Metrics">
                <i class="fa fa-chevron-left"></i> Flink Metrics
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/11/科学上网的秘籍/" rel="prev" title="科学上网的最终秘籍">
                科学上网的最终秘籍 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        
<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="SOHUCS"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="saixia" />
          <p class="site-author-name" itemprop="name">saixia</p>
           
              <p class="site-description motion-element" itemprop="description">Whatever is worth doing is worth doing well I'll think of you every step of the way</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">31</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/saixia" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度"><span class="nav-number">2.</span> <span class="nav-text">梯度</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#复合连锁法则"><span class="nav-number">3.</span> <span class="nav-text">复合连锁法则</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#直观理解反向传播"><span class="nav-number">4.</span> <span class="nav-text">直观理解反向传播</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Sigmod函数"><span class="nav-number">5.</span> <span class="nav-text">Sigmod函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#反向传播实例：计算过程"><span class="nav-number">6.</span> <span class="nav-text">反向传播实例：计算过程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#反向传播模式"><span class="nav-number">7.</span> <span class="nav-text">反向传播模式</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度计算中的向量运算"><span class="nav-number">8.</span> <span class="nav-text">梯度计算中的向量运算</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#总结"><span class="nav-number">9.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考文献"><span class="nav-number">10.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">saixia</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  




  

    <!-- 评论模块 -->
    <script type="text/javascript">

    (function(){
      var appid = 'cyt4Mszyk';
      var conf = 'prod_84012c6583611e5d040e9308852b5267';
      var width = window.innerWidth || document.documentElement.clientWidth;
      if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){
        window.changyan.api.config({appid:appid,conf:conf})});
      }
    })();

    </script>

    <!-- 评论统计模块 -->
    <script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script>




  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("LjhheWaehewK7q4K5t2VQv6W-gzGzoHsz", "xywKJ7E9yXzxFus7WnVTYStW");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

</body>
</html>
